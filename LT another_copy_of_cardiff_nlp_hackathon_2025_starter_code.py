# -*- coding: utf-8 -*-
"""Another copy of Cardiff NLP Hackathon 2025 - Starter Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eaFiGFKvbOxkScI7sjKoV3wK3dslYRKk

## Cardiff NLP Hackathon 2025 - Starter Code

Welcome to Cardiff NLP's second hackathon! Below is some code to get started on the AMPLYFI API and look at some data.

====================

Note: the API is a real time resource so extra points to projects that can treat it as a continual data stream rather than a one-off data source!

Another thing to note about this is that it will affect Amplyfi's servers if you download a silly amount of data. We ask that you only request 100 results per request, but if you have the data you need, try to download it or store it as a variable rather than requesting the exact same data over and over again.
"""

# Import some libraries

import requests
import json
import nltk
import re
import pandas as pd
from collections import defaultdict

from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

"""Amplyfi have provided some limits and explanations of what you can query the API for below:

`query_text` anything

`result_size` <=100

`include_highlights` boolean (if True, you get sentences matching keyphrases in the query)

`include_smart_tags` boolean (if True, you get back metadata from our "smart" market intel classifiers - signals and industries)

`ai_answer` can only accept "basic", this will take the 5 most relevant docs and answer the query_text based on them



"""

# API endpoint from the newly deployed service

API_URL = "https://zfgp45ih7i.execute-api.eu-west-1.amazonaws.com/sandbox/api/search"
API_KEY = "XYZ38746G38B7RB46GBER"

headers = {
    "Content-Type": "application/json",
    "x-api-key": API_KEY
}

# Edit the below to get different data
payload = {
  "query_text": "what is happening with riots in Los Angeles?",
  "result_size": 100,
  "include_highlights":True,
  "include_smart_tags":True,

}

response = requests.post(API_URL, headers=headers, data=json.dumps(payload))
json_response = response.json()

json_response

grouped_results = defaultdict(list)

for item in json_response['results']:

  if not item["timestamp"]:
    continue
  else:
    date = item["timestamp"].split("T")[0]

  grouped_results[date].append(item["title"])

grouped_results = dict(grouped_results)

sorted_group_counts = {date: len(grouped_results[date]) for date in sorted(grouped_results)}

#print(json.dumps(sorted_group_counts, indent=4))

print(json.dumps(grouped_results, indent=4))

from transformers import pipeline

pipe = pipeline("summarization", model="facebook/bart-large-cnn", max_length=30)

daily_summaries = []

query_results = json.dumps(grouped_results, indent=4)
#print(grouped_results)

for day in grouped_results:
  to_summarize = ""
  summaries = grouped_results[day] #['summary1', 'summary2'...]
  for summary in summaries:
    to_summarize += summary
  print(to_summarize)

# summaries_to_summarize = []

# for day in grouped_results: #[list of strings]
#   summaries = grouped_results[day]
#   to_summarize = ""
#   for summary in summaries:
#     to_summarize += summary


#   """
#   for summary in summaries:
#     to_summarize += summary
#   print(to_summarize)
#   """
# print(to_summarize)

# from transformers import pipeline

# summarizer = pipeline("summarization", model="facebook/bart-large-cnn", max_length=30)

# for day in grouped_results: #[list of strings]
#   print(day)
#   summaries = grouped_results[day]
#   to_summarize = ""
#   for summary in summaries:
#     to_summarize += summary
#   # print(to_summarize)
#   summary = summarizer(to_summarize, max_length=150, do_sample=False)
#   print(summary)

from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

for day, summaries in grouped_results.items():
    print(day)
    to_summarize = " ".join(summaries)[:2024]  # Limit input size
    summary = summarizer(to_summarize, max_length=100, num_beams=2, do_sample=False)
    print(summary[0]["summary_text"])

json_response['results'][0]

df = pd.json_normalize(json_response['results'])

df.head()

"""## Example Sentiment Analysis"""

## Clean data

def clean_text(text):
    """
    - Convert to lowercase
    - Remove URLs
    - Remove punctuation / non-alpha
    - Collapse multiple spaces
    """
    if not isinstance(text, str):
        return ""
    # Remove URLs (very basic)
    text = re.sub(r"http\S+|www\.\S+", "", text)
    # Lowercase
    text = text.lower()
    # Keep only letters and spaces
    text = re.sub(r"[^a-z\s]", " ", text)
    # Collapse multiple spaces
    text = re.sub(r"\s+", " ", text).strip()
    return text

df['clean_summary'] = df['summary'].apply(clean_text)

## Sentiment analysis example

sia = SentimentIntensityAnalyzer()

def get_sentiment_scores(text):
    """
    Returns a dict with these keys:
       - neg: negative sentiment score
       - neu: neutral score
       - pos: positive score
       - compound: normalized, weighted composite (-1 to +1)
    """
    return sia.polarity_scores(text)

# Apply to each summary
df['sentiment'] = df['clean_summary'].apply(get_sentiment_scores)

# Split into separate columns if you like
df['sent_neg'] = df['sentiment'].apply(lambda d: d['neg'])
df['sent_neu'] = df['sentiment'].apply(lambda d: d['neu'])
df['sent_pos'] = df['sentiment'].apply(lambda d: d['pos'])
df['sent_compound'] = df['sentiment'].apply(lambda d: d['compound'])

# Quick look at top 5 compound scores
print(df[['clean_summary', 'sent_compound']].sort_values(by='sent_compound', ascending=False).head())
print(df[['clean_summary', 'sent_compound']].sort_values(by='sent_compound').head())

# Find index of the most positive (max compound) and most negative (min compound) summaries
max_idx = df['sent_compound'].idxmax()
min_idx = df['sent_compound'].idxmin()

# Retrieve the scores
max_score = df.loc[max_idx, 'sent_compound']
min_score = df.loc[min_idx, 'sent_compound']

# Print the full clean summaries along with their sentiment scores
print("Most positive summary (compound = {:.3f}):\n".format(max_score))
print(df.loc[max_idx, 'clean_summary'])


print("\n\nMost negative summary (compound = {:.3f}):\n".format(min_score))
print(df.loc[min_idx, 'clean_summary'])

"""## Project Ideas

Feel free to use this code to start your own project, and here are some (Chat-GPT generated ðŸ˜¬) ideas for projects:

* Real-Time Sentiment Pulse: Visualize sentiment trends over the past 24-48 hours for any keyword.

* One-Click News Brief: Generate a 3-sentence summary of today's top articles on a given topic.

* Bias/Slant Detector: Compare headlines from multiple outlets on the same event and label their bias.

* Event Timeline Generator: Autofill a chronological list of key dates and summaries for any query.

* Breaking News Alert Bot: Push a short alert whenever article volume spikes or sentiment turns extreme.

* Multilingual Hashtag Trend Mapper: Show related hashtags and translations across different languages.

* Rumor vs. Fact Checker: Verify a user-provided statement against recent reputable sources.

* â€œWhat's Changed?â€ Comparator: Highlight how coverage of a topic has shifted from last month to last week.

* Geo-Mood Map: Color-code countries by average sentiment or topic intensity on a query.

* Voice-Activated News Q&A: Let users speak a question and hear back a 2â€“3 sentence summary of current events.

## Dashboard libraries for Python

https://shiny.posit.co/py/

https://dash.plotly.com/

https://streamlit.io/
"""

